
import gradio as gr
import os
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from dotenv import load_dotenv
import requests
import json
import time
import threading

# Load environment variables
load_dotenv()

try:
    from logger.custom_logger import CustomLoggerTracker
    custom_log = CustomLoggerTracker()
    logger = custom_log.get_logger("gradio_demo")

except ImportError:
    # Fallback to standard logging if custom logger not available
    logger = logging.getLogger("gradio_demo")



@dataclass
class Document:
    """Document structure for RAG system."""
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None

@dataclass
class RAGResult:
    """RAG query result."""
    query: str
    answer: str
    relevant_documents: List[Document]
    processing_time: float

class SimpleVectorStore:
    """Simple in-memory vector store for demonstration."""
    
    def __init__(self):
        self.documents: List[Document] = []
        self.embeddings: List[List[float]] = []
    
    def add_documents(self, documents: List[Document]):
        """Add documents to the vector store."""
        self.documents.extend(documents)
        for doc in documents:
            if doc.embedding:
                self.embeddings.append(doc.embedding)
    
    def similarity_search(self, query_embedding: List[float], top_k: int = 5) -> List[Document]:
        """Find most similar documents using cosine similarity."""
        if not self.embeddings or not query_embedding:
            return []
        
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            if doc_embedding:
                similarity = self._cosine_similarity(query_embedding, doc_embedding)
                similarities.append((similarity, self.documents[i]))
        
        # Sort by similarity and return top_k
        similarities.sort(key=lambda x: x[0], reverse=True)
        return [doc for _, doc in similarities[:top_k]]
    
    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """Calculate cosine similarity between two vectors."""
        if len(a) != len(b):
            return 0.0
        
        dot_product = sum(x * y for x, y in zip(a, b))
        norm_a = sum(x * x for x in a) ** 0.5
        norm_b = sum(x * x for x in b) ** 0.5
        
        if norm_a == 0 or norm_b == 0:
            return 0.0
        
        return dot_product / (norm_a * norm_b)

class SiliconFlowClient:
    """SiliconFlow API client for embeddings and chat completion."""
    
    def __init__(self, api_key: str, base_url: str = "https://api.siliconflow.cn/v1"):
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
    
    def generate_embeddings(self, texts: List[str], 
                          model: str = "BAAI/bge-large-zh-v1.5") -> List[List[float]]:
        """Generate embeddings for texts."""
        try:
            payload = {
                "model": model,
                "input": texts,
                "encoding_format": "float"
            }
            
            response = requests.post(
                f"{self.base_url}/embeddings",
                json=payload,
                headers=self.headers,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                return [item['embedding'] for item in data.get('data', [])]
            else:
                logger.error(f"Embedding API error: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            return []
    
    def rerank_documents(self, query: str, documents: List[str], 
                        model: str = "BAAI/bge-reranker-large",
                        top_k: int = 5) -> List[Dict]:
        """Rerank documents based on query relevance."""
        try:
            payload = {
                "model": model,
                "query": query,
                "documents": documents,
                "top_k": top_k,
                "return_documents": True
            }
            
            response = requests.post(
                f"{self.base_url}/rerank",
                json=payload,
                headers=self.headers,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                return data.get('results', [])
            else:
                logger.error(f"Rerank API error: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            logger.error(f"Reranking failed: {e}")
            return []
    
    def chat_completion(self, messages: List[Dict[str, str]], 
                       model: str = "Qwen/Qwen2.5-7B-Instruct") -> str:
        """Generate chat completion."""
        try:
            payload = {
                "model": model,
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 1000
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                headers=self.headers,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                return data['choices'][0]['message']['content']
            else:
                logger.error(f"Chat completion API error: {response.status_code} - {response.text}")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            logger.error(f"Chat completion failed: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

class RAGSystem:
    """Complete RAG system using SiliconFlow."""
    
    def __init__(self, api_key: str):
        self.client = SiliconFlowClient(api_key)
        self.vector_store = SimpleVectorStore()
        self.initialized = False
        logger.info("RAG System initialized")
    
    def add_documents(self, texts: List[str], metadatas: Optional[List[Dict]] = None):
        """Add documents to the RAG system."""
        if not metadatas:
            metadatas = [{"source": f"doc_{i}"} for i in range(len(texts))]
        
        logger.info(f"Adding {len(texts)} documents...")
        
        # Generate embeddings
        embeddings = self.client.generate_embeddings(texts)
        
        # Create document objects
        documents = []
        for i, (text, metadata) in enumerate(zip(texts, metadatas)):
            embedding = embeddings[i] if i < len(embeddings) else None
            doc = Document(content=text, metadata=metadata, embedding=embedding)
            documents.append(doc)
        
        # Add to vector store
        self.vector_store.add_documents(documents)
        self.initialized = True
        logger.info(f"Successfully added {len(documents)} documents")
    
    def query(self, query: str, top_k: int = 5, use_reranking: bool = True) -> RAGResult:
        """Query the RAG system."""
        start_time = time.time()
        
        if not self.initialized:
            return RAGResult(
                query=query,
                answer="ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                relevant_documents=[],
                processing_time=time.time() - start_time
            )
        
        # Generate query embedding
        query_embeddings = self.client.generate_embeddings([query])
        query_embedding = query_embeddings[0] if query_embeddings else []
        
        if not query_embedding:
            logger.error("Failed to generate query embedding")
            return RAGResult(
                query=query,
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                relevant_documents=[],
                processing_time=time.time() - start_time
            )
        
        # Find similar documents
        similar_docs = self.vector_store.similarity_search(query_embedding, top_k * 2)
        
        if not similar_docs:
            return RAGResult(
                query=query,
                answer="ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                relevant_documents=[],
                processing_time=time.time() - start_time
            )
        
        # Optional reranking
        if use_reranking and len(similar_docs) > top_k:
            doc_texts = [doc.content for doc in similar_docs]
            rerank_results = self.client.rerank_documents(query, doc_texts, top_k=top_k)
            
            if rerank_results:
                # Reorder documents based on reranking
                reranked_docs = []
                for result in rerank_results:
                    doc_idx = result.get('index', 0)
                    if doc_idx < len(similar_docs):
                        reranked_docs.append(similar_docs[doc_idx])
                similar_docs = reranked_docs
        
        # Limit to top_k
        relevant_docs = similar_docs[:top_k]
        
        # Generate answer using context
        context = "\n\n".join([doc.content for doc in relevant_docs])
        answer = self._generate_answer(query, context)
        
        processing_time = time.time() - start_time
        
        return RAGResult(
            query=query,
            answer=answer,
            relevant_documents=relevant_docs,
            processing_time=processing_time
        )
    
    def _generate_answer(self, query: str, context: str) -> str:
        """Generate answer using context and query."""
        system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ëŠ” ë„ì›€ì´ ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤'ë¼ê³  ë§í•´ì£¼ì„¸ìš”."""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ì»¨í…ìŠ¤íŠ¸:\n{context}\n\nì§ˆë¬¸: {query}"}
        ]
        
        return self.client.chat_completion(messages)

# Sample Korean manufacturing data
SAMPLE_DOCUMENTS = [
    "TAB S10 ë„ì¥ ê³µì •ì˜ ìˆ˜ìœ¨ì€ í˜„ì¬ 95.2%ì…ë‹ˆë‹¤. ëª©í‘œ ìˆ˜ìœ¨ 94%ë¥¼ ìƒíšŒí•˜ê³  ìˆìœ¼ë©°, ì§€ë‚œë‹¬ ëŒ€ë¹„ 1.3% í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.",
    "ë„ì¥ ë¼ì¸ì—ì„œ ë¶ˆëŸ‰ë¥ ì´ 4.8% ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ë¶ˆëŸ‰ ì›ì¸ì€ ì˜¨ë„ í¸ì°¨(45%)ì™€ ìŠµë„ ë³€í™”(30%)ì…ë‹ˆë‹¤.",
    "S10 ëª¨ë¸ì˜ ì „ì²´ ìƒì‚° ìˆ˜ìœ¨ì€ 89.5%ë¡œ ëª©í‘œì¹˜ 88%ë¥¼ ìƒíšŒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì›”ê°„ ìƒì‚°ëŸ‰ì€ 15,000ëŒ€ì…ë‹ˆë‹¤.",
    "ë„ì¥ ë¼ì¸ì˜ ì˜¨ë„ëŠ” 22Â±2â„ƒ, ìŠµë„ëŠ” 45Â±5%ë¡œ ìœ ì§€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ ìë™ ì œì–´ ì‹œìŠ¤í…œìœ¼ë¡œ ê´€ë¦¬ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
    "í’ˆì§ˆê´€ë¦¬ ë¶€ì„œì—ì„œëŠ” ë§¤ì¼ 3íšŒ ìƒ˜í”Œë§ ê²€ì‚¬ë¥¼ ì‹¤ì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê²€ì‚¬ í•­ëª©ì€ ìƒ‰ìƒ, ê´‘íƒ, ë‘ê»˜ì…ë‹ˆë‹¤.",
    "ì˜ˆë°© ë³´ì „ ê³„íšì— ë”°ë¼ ë„ì¥ ì„¤ë¹„ëŠ” ì£¼ 1íšŒ ì •ê¸° ì ê²€ì„ ì‹¤ì‹œí•©ë‹ˆë‹¤. ë‹¤ìŒ ì •ê¸° ë³´ì „ì€ ë‹¤ìŒ ì£¼ í™”ìš”ì¼ì…ë‹ˆë‹¤.",
    "ì‹ ê·œ ë„ì¥ ì¬ë£Œ ì ìš© í›„ ì ‘ì°©ë ¥ì´ 15% í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ë¹„ìš©ì€ 10% ì¦ê°€í–ˆì§€ë§Œ í’ˆì§ˆ ê°œì„  íš¨ê³¼ê°€ í½ë‹ˆë‹¤.",
    "ì‘ì—…ì êµìœ¡ì€ ì›” 2íšŒ ì‹¤ì‹œë˜ë©°, ì•ˆì „êµìœ¡ê³¼ í’ˆì§ˆêµìœ¡ì„ í¬í•¨í•©ë‹ˆë‹¤. êµìœ¡ ì°¸ì„ë¥ ì€ 98.5%ì…ë‹ˆë‹¤."
]

# Global RAG system instance
rag_system = None

def initialize_rag_system(api_key: str, progress=gr.Progress()) -> Tuple[str, str]:
    """Initialize the RAG system with the provided API key."""
    global rag_system
    
    if not api_key:
        return "âŒ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", "Not initialized"
    
    try:
        progress(0.1, desc="RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # Initialize RAG system
        rag_system = RAGSystem(api_key)
        
        progress(0.5, desc="ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        # Add sample documents
        rag_system.add_documents(SAMPLE_DOCUMENTS)
        
        progress(1.0, desc="ì™„ë£Œ!")
        
        return "âœ… RAG ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!", "Ready"
        
    except Exception as e:
        logger.error(f"RAG system initialization failed: {e}")
        return f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}", "Error"

def query_rag_system(query: str, top_k: int, use_reranking: bool, progress=gr.Progress()) -> Tuple[str, str, str]:
    """Query the RAG system and return results."""
    global rag_system
    
    if not rag_system or not rag_system.initialized:
        return "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € API í‚¤ë¥¼ ì…ë ¥í•˜ê³  ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.", "", ""
    
    if not query.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "", ""
    
    try:
        progress(0.2, desc="ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘...")
        progress(0.5, desc="ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
        progress(0.7, desc="ë¬¸ì„œ ì¬ìˆœìœ„í™” ì¤‘...")
        progress(0.9, desc="ë‹µë³€ ìƒì„± ì¤‘...")
        
        # Query the system
        result = rag_system.query(query, top_k=top_k, use_reranking=use_reranking)
        
        # Format relevant documents
        docs_text = ""
        for i, doc in enumerate(result.relevant_documents, 1):
            docs_text += f"**ë¬¸ì„œ {i}** ({doc.metadata.get('source', 'Unknown')})\n"
            docs_text += f"{doc.content}\n\n"
        
        # Format statistics
        stats_text = f"ğŸ• ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ\n"
        stats_text += f"ğŸ“„ ì°¾ì€ ë¬¸ì„œ ìˆ˜: {len(result.relevant_documents)}\n"
        stats_text += f"ğŸ”„ ì¬ìˆœìœ„í™” ì‚¬ìš©: {'Yes' if use_reranking else 'No'}\n"
        stats_text += f"ğŸ“Š Top-K: {top_k}"
        
        progress(1.0, desc="ì™„ë£Œ!")
        
        return result.answer, docs_text, stats_text
        
    except Exception as e:
        logger.error(f"Query failed: {e}")
        return f"âŒ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}", "", ""

def create_gradio_interface():
    """Create and configure the Gradio interface."""
    
    # Custom CSS for better styling
    css = """
    .container {
        max-width: 1200px;
        margin: 0 auto;
    }
    .header {
        text-align: center;
        margin-bottom: 20px;
    }
    .sample-questions {
        background-color: #f0f2f6;
        padding: 15px;
        border-radius: 10px;
        margin: 10px 0;
    }
    """
    
    with gr.Blocks(css=css, title="Korean RAG Demo", theme=gr.themes.Soft()) as demo:
        
        gr.HTML("""
            <div class="header">
                <h1>ğŸ¤– Korean RAG Demo with SiliconFlow</h1>
                <p><em>Retrieval-Augmented Generation for Korean Manufacturing Q&A</em></p>
            </div>
        """)
        
        with gr.Tab("ğŸ  Main"):
            with gr.Row():
                with gr.Column(scale=2):
                    # Configuration Section
                    gr.Markdown("## âš™ï¸ Configuration")
                    
                    api_key_input = gr.Textbox(
                        label="SiliconFlow API Key",
                        placeholder="Enter your SiliconFlow API key...",
                        type="password",
                        value=os.getenv("SILICONFLOW_API_KEY", "")
                    )
                    
                    with gr.Row():
                        init_button = gr.Button("ğŸš€ Initialize RAG System", variant="primary")
                        status_output = gr.Textbox(label="Status", value="Not initialized", interactive=False)
                    
                    init_output = gr.Markdown()
                    
                    # Settings
                    with gr.Accordion("ğŸ”§ Advanced Settings", open=False):
                        top_k_slider = gr.Slider(
                            minimum=1, maximum=10, value=3, step=1,
                            label="Top-K Results"
                        )
                        reranking_checkbox = gr.Checkbox(
                            label="Use Reranking", value=True
                        )
                
                with gr.Column(scale=3):
                    # Query Section
                    gr.Markdown("## ğŸ’¬ Ask Questions")
                    
                    # Sample questions
                    gr.HTML("""
                        <div class="sample-questions">
                            <h4>ğŸ“ Sample Questions:</h4>
                            <ul>
                                <li>TAB S10 ë„ì¥ ê³µì • ìˆ˜ìœ¨ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?</li>
                                <li>ë„ì¥ ë¼ì¸ì˜ ë¶ˆëŸ‰ë¥ ê³¼ ì£¼ìš” ì›ì¸ì€?</li>
                                <li>í’ˆì§ˆ ê²€ì‚¬ëŠ” ì–´ë–»ê²Œ ì§„í–‰ë˜ë‚˜ìš”?</li>
                                <li>ì˜ˆë°© ë³´ì „ ê³„íšì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”</li>
                            </ul>
                        </div>
                    """)
                    
                    query_input = gr.Textbox(
                        label="Your Question",
                        placeholder="ì˜ˆ: TAB S10 ìˆ˜ìœ¨ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                        lines=2
                    )
                    
                    # Quick question buttons
                    with gr.Row():
                        q1_btn = gr.Button("ìˆ˜ìœ¨ ì§ˆë¬¸", size="sm")
                        q2_btn = gr.Button("ë¶ˆëŸ‰ë¥  ì§ˆë¬¸", size="sm")
                        q3_btn = gr.Button("í’ˆì§ˆê²€ì‚¬ ì§ˆë¬¸", size="sm")
                        q4_btn = gr.Button("ë³´ì „ê³„íš ì§ˆë¬¸", size="sm")
                    
                    query_button = gr.Button("ğŸ” Search & Answer", variant="primary", size="lg")
            
            # Results Section
            gr.Markdown("## ğŸ“‹ Results")
            
            with gr.Row():
                with gr.Column(scale=2):
                    answer_output = gr.Markdown(label="Answer")
                
                with gr.Column(scale=1):
                    stats_output = gr.Markdown(label="Statistics")
            
            # Documents Section
            gr.Markdown("## ğŸ“„ Relevant Documents")
            documents_output = gr.Markdown()
        
        with gr.Tab("ğŸ“š Sample Documents"):
            gr.Markdown("## ğŸ“š Sample Manufacturing Documents")
            gr.Markdown("The system includes these Korean manufacturing documents:")
            
            sample_docs_text = ""
            for i, doc in enumerate(SAMPLE_DOCUMENTS, 1):
                sample_docs_text += f"**{i}.** {doc}\n\n"
            
            gr.Markdown(sample_docs_text)
        
        with gr.Tab("â„¹ï¸ About"):
            gr.Markdown("""
                ## ğŸ¤– About Korean RAG Demo
                
                This demo showcases a **Retrieval-Augmented Generation (RAG)** system specifically designed for Korean manufacturing Q&A.
                
                ### ğŸ”§ Features:
                - **Korean Language Support**: Optimized for Korean manufacturing terminology
                - **SiliconFlow Integration**: Uses BAAI embedding and reranking models
                - **Real-time Processing**: Fast query processing with similarity search
                - **Document Reranking**: Improves relevance with optional reranking
                - **Interactive Interface**: User-friendly Gradio interface
                
                ### ğŸš€ How it Works:
                1. **Document Embedding**: Sample documents are converted to vector embeddings
                2. **Query Processing**: Your question is also embedded using the same model
                3. **Similarity Search**: Find most relevant documents using cosine similarity
                4. **Reranking** (optional): Reorder results for better relevance
                5. **Answer Generation**: Generate contextual answers using retrieved documents
                
                ### ğŸ”‘ Requirements:
                - SiliconFlow API key with access to:
                  - `BAAI/bge-large-zh-v1.5` (embedding model)
                  - `BAAI/bge-reranker-large` (reranking model)
                  - `Qwen/Qwen2.5-7B-Instruct` (chat model)
                
                ### ğŸ“Š Sample Data:
                The demo includes 8 Korean manufacturing documents covering:
                - Production yield rates
                - Quality control processes
                - Defect analysis
                - Maintenance schedules
                - Training programs
            """)
        
        # Event handlers
        init_button.click(
            fn=initialize_rag_system,
            inputs=[api_key_input],
            outputs=[init_output, status_output]
        )
        
        query_button.click(
            fn=query_rag_system,
            inputs=[query_input, top_k_slider, reranking_checkbox],
            outputs=[answer_output, documents_output, stats_output]
        )
        
        # Sample question button handlers
        q1_btn.click(lambda: "TAB S10 ë„ì¥ ê³µì • ìˆ˜ìœ¨ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", outputs=query_input)
        q2_btn.click(lambda: "ë„ì¥ ë¼ì¸ì˜ ë¶ˆëŸ‰ë¥ ê³¼ ì£¼ìš” ì›ì¸ì€?", outputs=query_input)
        q3_btn.click(lambda: "í’ˆì§ˆ ê²€ì‚¬ëŠ” ì–´ë–»ê²Œ ì§„í–‰ë˜ë‚˜ìš”?", outputs=query_input)
        q4_btn.click(lambda: "ì˜ˆë°© ë³´ì „ ê³„íšì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”", outputs=query_input)
        
        # Allow Enter key to trigger search
        query_input.submit(
            fn=query_rag_system,
            inputs=[query_input, top_k_slider, reranking_checkbox],
            outputs=[answer_output, documents_output, stats_output]
        )
    
    return demo

def main():
    """Main function to run the Gradio app."""
    demo = create_gradio_interface()
    
    # Launch the app
    demo.launch(
        server_name="0.0.0.0",  # Allow external connections
        server_port=7860,       # Default Gradio port
        share=False,            # Set to True to create public link
        debug=True,             # Enable debug mode
        show_error=True         # Show detailed errors
    )

if __name__ == "__main__":
    main()